%
% Author: Nick Richardson
% TODO:
%	  -
%
%
%
%
%

\documentclass[10pt]{article}
\usepackage[english]{babel}
\usepackage{lineno}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{tocloft}
\usepackage{marginnote}
\newenvironment{changemargin}[2]{%
\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{#1}%
\setlength{\rightmargin}{#2}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{\parskip}%
}%
\item[]}{\end{list}}
\usepackage{marginnote}
\usepackage[top=1.5cm, bottom=1.5cm, outer=6cm, inner=3cm, heightrounded, marginparwidth=2.5cm, marginparsep=1cm]{geometry}
% \newcommand{\bf}{\textbf}
% \newcommand{\sf}{\textsf}
%\newcommand{\em}{\textem}
% \newcommand{\sc}{\textsc}
\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist

\renewcommand{\it}{\textit}


\begin{document}

\begin{changemargin}{+-1cm}{-3cm}
\noindent
\large{\textbf{Proposal}}\\

\large{Machine Intelligence: Applied Mathematics and Machine Learning}\\
\small

\noindent
\textit{Authors}
\begin{changemargin}{1cm}{0cm}
\textbf{Ruye Wang} is a Professor in the Engineering Department at Harvey Mudd College. Previously a Principal Investigator at the Jet Propulsion Laboratory, NASA, his research interests include image processing, computer vision, machine learning and remote sensing. Ruye's teaching spans machine learning, electrical engineering, and applied mathematics, and is the author of \textit{Introduction to Orthogonal Transforms With Applications in Data Processing and Analysis}, (Cambridge University Press, 2012). He is the principal investigator of multiple research grants, including a NASA research grant of \$500,000. His collective works in image processing, pattern recognition, and signal processing have receieved over 1,500 citations.

\begin{center}
340 E. Foothill Blvd., Claremont, CA 91711 $\cdot$ rwang@g.hmc.edu $\cdot$ (555) - 555 - 5555 \\
\end{center}

\textbf{Nick Richardson} is a fourth-year undergraduate at Harvey Mudd College studying applied mathematics. His interests within machine learning include Bayesian nonparameterics, metric learning, applied information theory, and differential geometry. Nick has served as class president and student government chair at Harvey Mudd, as well as a tutor in the engineering, computer science, and mathematics departments. He was head teaching assistant and guest lecturer for engineering 190, an upper division machine learning course tailored for junior and senior engineering majors. Nick has worked as a machine learning engineer/applied mathematician, and currently works remotely from school as a machine learning consultant to a biometrics startup in Redwood City, CA.

\begin{center}
340 E. Foothill Blvd. \# 544, Claremont, CA 91711 $\cdot$ nrichardson@g.hmc.edu $\cdot$ (708) - 979 - 1500
\end{center}

\end{changemargin}

\vspace{1cm}
\noindent
\large{Background}\\
\small

\noindent
The pursuit of machine intelligence (AI or MI) in recent decades has been dominated by a largly \it{statistical} approach to achieve adaptative systems which can learn from data and reason under uncertainty. The aggregation of immense datasets, increased computational capacities, and modest theoretical advancements have cultivated a fascinating intersection of applied mathematics and computer science; the field of machine learning. It is difficult to overstate the current public excitement surrounding the development of commercially valuable MI systems. It is incontrovertible that we lie on a trajectory toward increasing the level of sophistication and breadth of application areas, which ideally will enhance our economic and scientific productivity. It is worth noting that we do not regard machine learning or MI as unique or special in this regard, but simply as one of several (e.g., genetic engineering, quantum computing) exciting technological developments beginning to mature. \\

Just several decades ago, enthusiasts in fields like statistics, optimization, statistical physics, signal processing, oand operations research viewed machine learning research as reinventing the wheel \footnote{Sentiment taken from (Wasserman, 2004)}. Likewise, the field of machine learing did not initally comprehend the important contributions made by these other fields as applied to their own work in prediction and pattern recognition. In the modern era, however, the sibling fields noted above are recognizing the novel contributions by machine learning, and machine learning researchers are utilizing the intellectual foundation of myriad disciplines (many previously thought irrelevant). It is in this sense that we say machine learning crystalized from both science and engineering. That is, there is room to view machine learning on the one hand as the application of rigorous tools from applied mathematics, and on the other as an`art' dedicated to building systems at the cutting edge of empirical power, without mention of proofs or bounds regarding the behaviors of these systems. Subfields of machine learning like Bayesian inference and deep learning seemingly compete with one another for the notion of a `correct' perspective on machine learning. All the while, the realization of commercial value in machine learning and related fields has drastically increased industrial/public attention. The consequence of this attention is an immense corpus of pedagogical resources: textbooks, so-called `bootcamps', and courses (online or otherwise), to teach machine learning. \\

One thing is perhaps clear: empirically powerful engineered systems will continue to be applied to a growing variety of tasks, and many students of these methods will apply these methods without a sophisticated understanding of their boundaries. As we allude to above, we believe the historical dichotomy between rigor and empiricism in machine learning is superficial at best. Rigor should not be synonymous with theory, nor empiricism with ad-hocery. Our aim with this text is to provide a novel view on teaching machine learning; taking a student all the way from applied mathematics to practical considerations in the construction of machine intelligence systems. We take `rigor' to mean doing scientifically reproducible and systematic machine learning, and `empiricism' to mean rationality in our engineering design decisions.\\

 With this in mind, we've set out to capture the \it{fundamentals} of machine learning. Like any reasonably mature intellectual enterprise, machine learning cannot be `understood' or `learned' in a single book, any more than photography or economics. Instead, we set out to cover the central concepts of machine learning at depth, giving the reader the confidence and vocabulary to pursue further study from whichever department they reside.

\vspace{1cm}
\noindent
\large{Readership}\\
\small

This book is intended for undergraduates in mathematics, computing, engineering, and science. It can be used as a resource for a course in machine learning, pattern recognition, or inference, in the mathematics, computer science, or engineering departments. Several roadmaps are provided for possible course outlines. In addition, the software included with the book is meant to serve two communities. First, for the student. Working with these tools in a hands-on manner, at the correct level of abstraction, is invaluable as a pedagogical tool. Second, for the practitioner. The software we provide (as well as the software we point to) may serve as a reference for new or senior professionals working in related industries. It was with this second community in mind that we chose to write the software in both the Python and Matlab languages; Matlab for more senior professionals coming from an engineering background, and the corresponding Python code in an attempt to modernize the craft. \\


\vspace{1cm}
\noindent
\large{Brief Description of the Book}\\
\small

This book, in short, is concerned with the methods required to understand and build systems which learn their behaviors from data. That is, engineered systems which can autonomously recover structure in data, and utilize that strcture to make inferences about the world. The book connects concepts from applied mathematics and mathematical inference with the practical engineering guidelines required to solve difficult problems in the real world. \\

The range of topics presented in the book was designed with the intent of introducing readers to the core ideas in machine learning; it is not meant to be encyclopaedic. As the book is intended for undergraduates, we do not assume a rich knowledge of linear algebra, probability theory, or signal processing. Instead, we introduce the important parts of these fields (important here meant with respect to machine learning) in the first section of the book: Foundations. Rather than introducing definitions outside of context, the core ideas from these foundational fields of applied math are introduced with sample applications, and hinting all the way at later machine learning applications to be introduced later in the text. Some examples of these hints are given in \it{Why another Machine Learning Textbook?}.

As we see it, there are two primary paths to gaining experience in machine learning as an undergraduate. The first is immersion in textbooks and courses on statistics, probability theory, optimization, and information theory. This student might indeed pick up a copy of a large graduate level tome like (Murphy 2010) or (Bishop 2006). Barring industry experience, however, this student often finds herself less confident in her ability to \it{apply} this theoretical foundation. The second path is to attend `data science' coding camps, online courses, or online competitions (e.g., Kaggle). This student is often quickly able to attend to problems using premade packages, but often lacks a deep understanding of the methods being used, nor the effects of her design decisions. It it our opinion that neither of these paths is pedagogically sensible. On one hand, the genesis of some the greatest insights and connections is when one attempts to \it{apply} theory, hence many of the wonderful discoveries through applied mathematics. On the other, one cannot hope to generalize to novel or serious engineering challenges without understanding the machinery of the fantastic packages available. \\

Our book attempts to ride the edge between providing a hearty theoretical foundation and genuinely useful advice on writing programs for these methods. We write our own software package to complement the mathematical presentation in the book, and illustrate specific conceptual details that might otherwise be lost by a purely theoretical explanation. As a concrete example, we discuss the theoretical discussion of bayesian versus frequentist methods, but more importantly, design software exercises and demos that illustrate the differences in an actual application domain. We write these programs in both Matlab and Python, with the intention that first learners are exposed to two tools (it's always valuable to see an idea expressed in two languages), and so that senior researchers might see familiar Matlab implementations translated to the currently dominant language for data analysis. \\

In terms of content, the book is partitioned into the following sections: \it{Foundations}, \it{Representation},  \it{Inference}, \it{Application}, and \it{Frontiers}. The Foundations material covers probabilities and inference, linear algebra, and basic signal processing, all as applied to machine learning. The Representation section is concerned with constructing an effective basis for a set of data. This section is intended as the intellectual workhorse of the book, introducing basis function expansions, adaptive basis function methods, unsupervised learning methods, metric learning, and several examples of data representation. We mean `workhorse' in the sense that we introduce topics like neural networks as adaptive basis function methods here, arguing that the central contribution of these methods is the construction of effective data representation, and that performing inference with these systems is simply extending their representational capacity with a linear regressor or classifier on that learned basis. In this sense, the Inference section describes the use of these basis representations to make predictions, typically with linear models like linear and logistic regression, which are discussed at length. Furthermore, the Application section engages a variety of practical questions concerning a machine learning engineer or researcher: How does one begin a machine learning project? How does one explore a dataset and understand the difficulty of a given problem? Once a problem has been specified, how can one choose a set of methods to engage with the problem, and then choose again among that set for the final system? How does one choose so called `hyperparameters'? None of these questions have \it{answers} in any precise sense, but practical advice and guidelines are given on these topics and others. Finally, Frontiers in machine learning and its applications are discussed. Sure to be out of date quickly, as any `Frontiers' section ought to be in a growing scientific field, we discuss here important applications of machine learning to modern problems: autonomy in vehicles, medical robotics, and weapon systems, the interfacing of biological neural systems with computational ones, and ponderings on the quest for machine intelligence.

\vspace{1cm}
\noindent
\large{Why \it{another} Machine Learning Textbook?}\\
\small

Unlike many of the more popular machine learning books available (Murphy 2015, Bishop 2006, Hastie et. al 2001), this book does not assume or require a great deal of mathematical maturity. The primary feature of our text will be its well-paced and clear bridging of the applied mathematics and the practical engineering. To be precise, we introduce all of the basic mathematics required to understand later sections of the book in the preliminary Foundations portion, and feel we have done so in a unique and effective manner. Rather than parroting the typical theorems and definitions given in a book on mathematics, each section in Foundations provides hints and nods to later content, giving the reader several visits to the same idea, within the same textbook. Some examples of these hints are given below:

\begin{itemize}
  \item In the section on probability, we introduce point estimation (maximum likelihood and maximum a posteriori), markov chain monte carlo, and the language of Bayesian inference.
  \item In the section on linear algebra, we introduce $k$-means clustering as a taste of unsupervised learning, compositions of affine functions as a preview to neural networks (which simply place a nonlinearity between each affine function), the singular value decomposition as background for principal components analysis, and the least squares problem as a noise-model-agnostic interpretation of linear regression.
  \item In the section on signal processing, we introduce the Kalman filter and the Hidden Markov model, later discussed in the context of latent variable models. We also introduce many basic data preprocessing and exploratory data analysis steps like rescaling, centering, histograms, viewing the decay of the data spectrum, filtering, etc.
\end{itemize}

In addition to our approach in presenting machine learning fundamentals in a palatable and comprehensive way, we feel our committment toward practical guidelines and advice is an even more powerful departure from current texts. In particular, we have written an entire section (one of five in the text) on machine learning applications, providing a complete beginner with a small guidebook to her machine learning problem. In that section we engage with a variety of common questions (see \it{Brief Description of the Book}), as well as providing several sample machine learning projects with the accompanying code (in the form of documented Jupyter notebooks) to follow along and replicate the results. \\

This book is in one sense a reaction to and a unification of machine learning as a field of science and engineering for the next generation in the field. The book does not represent a contribution to the debate, per se, but an understanding of the value in both perspectives. To the scientist, we write so that she is able to be rigorous and powerful in the \it{application} of her theory. To the engineer, we write so that she may gain a rich \it{understanding} of the powers, the weaknesses, and the boundaries of the rapidly expanding set of tools at her disposal. \\

\newpage
\noindent
\large{Competition}\\
\small

\textbf{Books Marketed Towards Senior Undergraduates} \\

$\oint$ \it{Machine Learning: A Probabilistic Perspective}, Kevin Murphy \\
$\oint$ \it{Pattern Recognition and Machine Learning}, Christopher Bishop \\
$\oint$ \it{The Elements of Statistical Learning}, Hastie et al. \\

Of these texts, our book differs strongly in its expectations of mathematical maturity in the reader. Unlike (Murphy 2010), we do not attempt to cover anywhere near the entirety of the field. We also do not assume a full course in statistics, or an understanding of linear regression, as does ESL. Our book covers a smaller number of methods, with an emphasis on a thorough explanation of these methods and the accompanying applications. \\

\textbf{Books Marketed Towards Junior Undergraduates} \\

$\oint$ \it{Foundations of Machine Learning}, Mehryar Mohri et al. \\

Mohri's book takes a particular `Learning Theory' flavor that we do not. For example, entire sections of Mohri's book are dedicated to PAC learning, VC-dimensions, and Learning Automata and Languages. Mohri's text also does not seem to follow a unified theme of ideas, moving quickly from learning theory, to ranking problems, then to online learning and eventually reinforcement learning. Though we find all of these topics important, our aim is to write a book that presents a thorough vision of a few parts of the field, rather than a little of everything.\\

$\oint$ \it{Hands-On Machine Learning with Scikit-Learn and TensorFlow}, Aurelien Geron \\

While this book does an excellent job introducing the reader to the practical tools in machine learning, it lacks the technical depth we aim for in our book.\\

\end{changemargin}
\end{document}
